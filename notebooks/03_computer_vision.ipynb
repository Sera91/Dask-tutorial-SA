{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you’ll learn here:\n",
    "\n",
    "    How to run inference tasks with Pytorch on the GPU cluster\n",
    "    How to use batch processing to accelerate your inference tasks with Pytorch on the GPU cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we need to ensure that our image dataset is available and that our GPU cluster is running.\n",
    "\n",
    "In our case, we have stored the data on S3 and use the s3fs library to work with it, as you’ll see below. If you would like to use this same dataset, it is the Stanford Dogs dataset, available here: http://vision.stanford.edu/aditya86/ImageNetDogs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "\n",
    "client.run(lambda: torch.cuda.is_available())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we set the device for pytorch computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Inference\n",
    "\n",
    "Now, we’re ready to start doing some classification! We’re going to use some custom-written functions to do this efficiently and make sure our jobs can take full advantage of the parallelization of the GPU cluster.\n",
    "\n",
    "\n",
    "#Preprocessing\n",
    "Single Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def preprocess(path, fs=__builtins__):\n",
    "    '''Ingest images directly from S3, apply transformations,\n",
    "    and extract the ground truth and image identifier. Accepts\n",
    "    a filepath. '''\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(250),\n",
    "        transforms.ToTensor()])\n",
    "\n",
    "    with fs.open(path, 'rb') as f:\n",
    "        img = Image.open(f).convert(\"RGB\")\n",
    "        nvis = transform(img)\n",
    "\n",
    "    truth = re.search('dogs/Images/n[0-9]+-([^/]+)/n[0-9]+_[0-9]+.jpg', path).group(1)\n",
    "    name = re.search('dogs/Images/n[0-9]+-[a-zA-Z-_]+/(n[0-9]+_[0-9]+).jpg', path).group(1)\n",
    "\n",
    "    return [name, nvis, truth]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows us to process one image, but of course, we have a lot of images to work with here! We’re going to use some list comprehension strategies to create our batches and get them ready for our inference.\n",
    "\n",
    "First, we break the list of images we have from our S3 file path into chunks that will define the batches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3fpath = 's3://dask-datasets/dogs/Images/*/*.jpg'\n",
    "\n",
    "batch_breaks = [list(batch) for batch in toolz.partition_all(60, s3.glob(s3fpath))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pred_batch(batch, gtruth, classes):\n",
    "    ''' Accepts batch of images, returns human readable predictions. '''\n",
    "    _, indices = torch.sort(batch, descending=True)\n",
    "    percentage = torch.nn.functional.softmax(batch, dim=1)[0] * 100\n",
    "\n",
    "    preds = []\n",
    "    labslist = []\n",
    "    for i in range(len(batch)):\n",
    "        pred = [(classes[idx], percentage[idx].item()) for idx in indices[i][:1]]\n",
    "        preds.append(pred)\n",
    "\n",
    "        labs = gtruth[i]\n",
    "        labslist.append(labs)\n",
    "\n",
    "    return(preds, labslist)\n",
    "\n",
    "def is_match(la, ev):\n",
    "    ''' Evaluate human readable prediction against ground truth.\n",
    "    (Used in both methods)'''\n",
    "    if re.search(la.replace('_', ' '), str(ev).replace('_', ' ')):\n",
    "        match = True\n",
    "    else:\n",
    "        match = False\n",
    "    return(match)\n",
    "\n",
    "\n",
    "@dask.delayed\n",
    "def run_batch_to_s3(iteritem):\n",
    "    ''' Accepts iterable result of preprocessing,\n",
    "    generates inferences and evaluates. '''\n",
    "\n",
    "    with s3.open('s3://dask-datasets/dogs/imagenet1000_clsidx_to_labels.txt') as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    names, images, truelabels = iteritem\n",
    "\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Set up model\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        resnet = resnet.to(device)\n",
    "        resnet.eval()\n",
    "\n",
    "        # run model on batch\n",
    "        images = images.to(device)\n",
    "        pred_batch = resnet(images)\n",
    "\n",
    "        #Evaluate batch\n",
    "        preds, labslist = evaluate_pred_batch(pred_batch, truelabels, classes)\n",
    "\n",
    "        #Organize prediction results\n",
    "        for j in range(0, len(images)):\n",
    "            predicted = preds[j]\n",
    "            groundtruth = labslist[j]\n",
    "            name = names[j]\n",
    "            match = is_match(groundtruth, predicted)\n",
    "\n",
    "            outcome = {'name': name, 'ground_truth': groundtruth, 'prediction': predicted, 'evaluation': match}\n",
    "\n",
    "            # Write each result to S3 directly\n",
    "            with s3.open(f\"s3://dask-datasets/dogs/preds/{name}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(outcome, f)\n",
    "\n",
    "        return(names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start the Dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = client.map(run_batch_to_s3, image_batches)\n",
    "futures_gathered = client.gather(futures)\n",
    "futures_computed = client.compute(futures_gathered, sync=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With map we ensure all our batches will get the function applied to them. With gather, we can collect all the results simultaneously rather than one by one. With compute(sync=False) we return all the futures, ready to be calculated when we want them. This may seem arduous, but these steps are required to allow us to iterate over the future.\n",
    "\n",
    "Now we actually run the tasks, and we also have a simple error handling system just in case any of our files are messed up or anything goes haywire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "results = []\n",
    "errors = []\n",
    "for fut in futures_computed:\n",
    "    try:\n",
    "        result = fut.result()\n",
    "    except Exception as e:\n",
    "        errors.append(e)\n",
    "        logging.error(e)\n",
    "    else:\n",
    "        results.extend(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate\n",
    "We want to make sure we have high-quality results coming out of this model, of course! First, we can peek at a single result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with s3.open('s3://dask-datasets/dogs/preds/n02086240_1082.pkl', 'rb') as data:\n",
    "    old_list = pickle.load(data)\n",
    "    old_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310-2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3f3bc039fcb8b99c0f943c0d90421db50d5a3e005969ab2b561163d4d9c3b99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
